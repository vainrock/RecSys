{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1653715891921,
     "user": {
      "displayName": "Nikita Vinokurov",
      "userId": "07709562798396934626"
     },
     "user_tz": -540
    },
    "id": "tD4awBei7AV4"
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "# Set shell to show all lines of output\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1513,
     "status": "ok",
     "timestamp": 1653716186870,
     "user": {
      "displayName": "Nikita Vinokurov",
      "userId": "07709562798396934626"
     },
     "user_tz": -540
    },
    "id": "387QiqEn_PHV",
    "outputId": "21d7f2c9-3121-4ed9-e4e4-2ee39a9c81e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 37020 books.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "books = []\n",
    "\n",
    "with open('C:/Users/Nikita/Documents/uni shit/Диплом/downloadwikipedia/data/filtered_list_films.ndjson', 'r') as fin:\n",
    "    # Append each line to the books\n",
    "    books = [json.loads(l) for l in fin]\n",
    "\n",
    "# Remove non-book articles\n",
    "books_with_wikipedia = [book for book in books if 'Wikipedia:' in book[0]]\n",
    "books = [book for book in books if 'Wikipedia:' not in book[0]]\n",
    "print(f'Found {len(books)} books.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1653716186871,
     "user": {
      "displayName": "Nikita Vinokurov",
      "userId": "07709562798396934626"
     },
     "user_tz": -540
    },
    "id": "f035OMZUVmZm",
    "outputId": "1205231e-7302-4cfc-9c51-c1f450d36f56"
   },
   "outputs": [],
   "source": [
    "[book[0] for book in books_with_wikipedia][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 366,
     "status": "ok",
     "timestamp": 1653716187230,
     "user": {
      "displayName": "Nikita Vinokurov",
      "userId": "07709562798396934626"
     },
     "user_tz": -540
    },
    "id": "XzySHf8iVpav",
    "outputId": "63aa58f6-115e-46be-cbb8-8d2b7cb7764e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Limonov (novel)',\n",
       " {'name': 'Limonov',\n",
       "  'author': 'Emmanuel Carrère',\n",
       "  'translator': 'John Lambert',\n",
       "  'country': 'France',\n",
       "  'language': 'French',\n",
       "  'publisher': 'P.O.L.',\n",
       "  'pub_date': '2011',\n",
       "  'english_pub_date': '2014',\n",
       "  'pages': '488',\n",
       "  'isbn': '978-2-8180-1405-9'},\n",
       " ['Emmanuel Carrère',\n",
       "  'biographical novel',\n",
       "  'Emmanuel Carrère',\n",
       "  'Eduard Limonov',\n",
       "  'Prix de la langue française'],\n",
       " ['http://www.lefigaro.fr/flash-actu/2011/10/05/97001-20111005FILWWW00615-le-prix-de-la-langue-francaise-a-e-carrere.php',\n",
       "  'http://www.lexpress.fr/culture/livre/emmanuel-carrere-prix-renaudot-2011_1046819.html',\n",
       "  'http://limonow.de/carrere/index.html',\n",
       "  'http://www.tout-sur-limonov.fr/222318809'],\n",
       " ['http://www.lefigaro.fr/flash-actu/2011/10/05/97001-20111005FILWWW00615-le-prix-de-la-langue-francaise-a-e-carrere.php',\n",
       "  'http://www.lexpress.fr/culture/livre/emmanuel-carrere-prix-renaudot-2011_1046819.html',\n",
       "  'http://limonow.de/carrere/index.html',\n",
       "  'http://www.tout-sur-limonov.fr/222318809'],\n",
       " '2018-08-18T02:03:21Z',\n",
       " 1437)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 21\n",
    "books[n][0], books[n][1], books[n][2][:5], books[n][3][:5], books[n][3][:5], books[n][4], books[n][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1653716187230,
     "user": {
      "displayName": "Nikita Vinokurov",
      "userId": "07709562798396934626"
     },
     "user_tz": -540
    },
    "id": "H-BttoXsVrw9",
    "outputId": "9776a042-e62d-47a3-893f-ec4bbe3b050a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22494"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Anna Karenina'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_index = {book[0]: idx for idx, book in enumerate(books)}\n",
    "index_film = {idx: book for book, idx in book_index.items()}\n",
    "\n",
    "book_index['Crime and Punishment']\n",
    "index_book[22494]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 480,
     "status": "ok",
     "timestamp": 1653716187703,
     "user": {
      "displayName": "Nikita Vinokurov",
      "userId": "07709562798396934626"
     },
     "user_tz": -540
    },
    "id": "3G9I_9e2VvY0",
    "outputId": "5ae9615a-d030-471c-b2c3-935029ef9f0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 311276 unique wikilinks.\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "wikilinks = list(chain(*[book[2] for book in books]))\n",
    "print(f\"There are {len(set(wikilinks))} unique wikilinks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 453,
     "status": "ok",
     "timestamp": 1653716188153,
     "user": {
      "displayName": "Nikita Vinokurov",
      "userId": "07709562798396934626"
     },
     "user_tz": -540
    },
    "id": "UDp4u5f0Vx0R",
    "outputId": "b80b302a-3523-4596-c081-eb9cc03de800"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 17032 unique wikilinks to other books.\n"
     ]
    }
   ],
   "source": [
    "wikilinks_other_films = [link for link in wikilinks if link in film_index.keys()]\n",
    "print(f\"There are {len(set(wikilinks_other_films))} unique wikilinks to other films.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1653716188153,
     "user": {
      "displayName": "Nikita Vinokurov",
      "userId": "07709562798396934626"
     },
     "user_tz": -540
    },
    "id": "P2MyWRoLV0SK"
   },
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "\n",
    "def count_items(l):\n",
    "    \"\"\"Return ordered dictionary of counts of objects in `l`\"\"\"\n",
    "    \n",
    "    # Create a counter object\n",
    "    counts = Counter(l)\n",
    "    \n",
    "    # Sort by highest count first and place in ordered dictionary\n",
    "    counts = sorted(counts.items(), key = lambda x: x[1], reverse = True)\n",
    "    counts = OrderedDict(counts)\n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1201,
     "status": "ok",
     "timestamp": 1653716189350,
     "user": {
      "displayName": "Nikita Vinokurov",
      "userId": "07709562798396934626"
     },
     "user_tz": -540
    },
    "id": "K60bBh8EV26Q",
    "outputId": "99518eb1-1533-4278-f27b-a21ca9f408b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hardcover', 7489),\n",
       " ('Paperback', 7311),\n",
       " ('Wikipedia:WikiProject Books', 6043),\n",
       " ('Wikipedia:WikiProject Novels', 6015),\n",
       " ('English language', 4185),\n",
       " ('United States', 3060),\n",
       " ('Science fiction', 3030),\n",
       " ('The New York Times', 2727),\n",
       " ('science fiction', 2502),\n",
       " ('novel', 1979)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find set of wikilinks for each film and convert to a flattened list\n",
    "unique_wikilinks = list(chain(*[list(set(film[2])) for film in films]))\n",
    "\n",
    "wikilink_counts = count_items(unique_wikilinks)\n",
    "list(wikilink_counts.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1363,
     "status": "ok",
     "timestamp": 1653716190709,
     "user": {
      "displayName": "Nikita Vinokurov",
      "userId": "07709562798396934626"
     },
     "user_tz": -540
    },
    "id": "urSv8jQQV5HX",
    "outputId": "7c80b687-9e98-4662-f13d-1627ada461d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 297624 unique wikilinks.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('paperback', 8740),\n",
       " ('hardcover', 8648),\n",
       " ('wikipedia:wikiproject books', 6043),\n",
       " ('wikipedia:wikiproject novels', 6016),\n",
       " ('science fiction', 5665),\n",
       " ('english language', 4248),\n",
       " ('united states', 3063),\n",
       " ('novel', 2983),\n",
       " ('the new york times', 2742),\n",
       " ('fantasy', 2003)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikilinks = [link.lower() for link in unique_wikilinks]\n",
    "print(f\"There are {len(set(wikilinks))} unique wikilinks.\")\n",
    "\n",
    "wikilink_counts = count_items(wikilinks)\n",
    "list(wikilink_counts.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 361,
     "status": "ok",
     "timestamp": 1653716191067,
     "user": {
      "displayName": "Nikita Vinokurov",
      "userId": "07709562798396934626"
     },
     "user_tz": -540
    },
    "id": "y8Tbe-4WWQgG",
    "outputId": "f070c28e-b207-4587-df08-20c9de55edcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41764\n"
     ]
    }
   ],
   "source": [
    "# Limit to greater than 3 links\n",
    "links = [t[0] for t in wikilink_counts.items() if t[1] >= 4]\n",
    "print(len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 443,
     "status": "ok",
     "timestamp": 1653716191508,
     "user": {
      "displayName": "Nikita Vinokurov",
      "userId": "07709562798396934626"
     },
     "user_tz": -540
    },
    "id": "MlHC3vNWV9eQ",
    "outputId": "970dfef8-9c8f-46b7-e0e0-decd56c6078c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The Encyclopedia of Science Fiction', 127),\n",
       " ('The Discontinuity Guide', 104),\n",
       " ('The Encyclopedia of Fantasy', 63),\n",
       " ('Dracula', 55),\n",
       " ('Encyclopædia Britannica', 51),\n",
       " ('Nineteen Eighty-Four', 51),\n",
       " ('Don Quixote', 49),\n",
       " ('The Wonderful Wizard of Oz', 49),\n",
       " (\"Alice's Adventures in Wonderland\", 47),\n",
       " ('Jane Eyre', 39)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find set of book wikilinks for each book\n",
    "unique_wikilinks_films = list(chain(*[list(set(link for link in film[2] if link in film_index.keys())) for film in films]))\n",
    "\n",
    "# Count the number of books linked to by other books\n",
    "wikilink_film_counts = count_items(unique_wikilinks_films)\n",
    "list(wikilink_book_counts.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1653716191509,
     "user": {
      "displayName": "Nikita Vinokurov",
      "userId": "07709562798396934626"
     },
     "user_tz": -540
    },
    "id": "BDcCwfQoWAPc",
    "outputId": "6dd597ca-546d-43b1-866f-5710f29f11a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Big Picture: Who Killed Hollywood? and Other Essays ['Wikipedia:WikiProject Novels', 'Wikipedia:WikiProject Books', 'William Goldman', 'United States', 'English language', 'William Goldman', 'Michael Sragow', 'Good Will Hunting', 'Robin Williams', 'Matt Damon', 'The New York Times', 'The New York Times Company', 'New York Times', 'Category:Cinema of the United States', 'Category:Film production', 'Category:2000 books', 'Category:Books about films', 'Category:Books by William Goldman', 'Category:Show business memoirs']\n"
     ]
    }
   ],
   "source": [
    "for film in films:\n",
    "    if 'The New York Times' in film[2] and 'New York Times' in film[2]:\n",
    "        print(film[0], film[2])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1653716191509,
     "user": {
      "displayName": "Nikita Vinokurov",
      "userId": "07709562798396934626"
     },
     "user_tz": -540
    },
    "id": "HPEnXPRXWCDi",
    "outputId": "fe0cbde4-75ae-4f4b-adcd-07e09836d937"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2742"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "996"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikilink_counts.get('the new york times')\n",
    "wikilink_counts.get('new york times')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 368,
     "status": "ok",
     "timestamp": 1653716191872,
     "user": {
      "displayName": "Nikita Vinokurov",
      "userId": "07709562798396934626"
     },
     "user_tz": -540
    },
    "id": "TyPrd2cqWD3R",
    "outputId": "fd51605d-9641-489c-e16c-81738bafb552"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "306"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'financial times'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 41764 wikilinks that will be used.\n"
     ]
    }
   ],
   "source": [
    "link_index = {link: idx for idx, link in enumerate(links)}\n",
    "index_link = {idx: link for link, idx in link_index.items()}\n",
    "\n",
    "link_index['the economist']\n",
    "index_link[300]\n",
    "print(f'There are {len(link_index)} wikilinks that will be used.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "J9d9yb0hWVpR"
   },
   "outputs": [],
   "source": [
    "pairs = []\n",
    "\n",
    "# Iterate through each book\n",
    "for film in films:\n",
    "    # Iterate through the links in the book\n",
    "    pairs.extend((film_index[film[0]], link_index[link.lower()]) for link in film[2] if link.lower() in links)\n",
    "    \n",
    "len(pairs), len(links), len(films)\n",
    "pairs[5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "xjv-S6SkWcmG"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The Parallax View (book)', 'antonio negri')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_film[pairs[5000][0]], index_link[pairs[5000][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "XGsQXxt-Wh13"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Historia Plantarum (Theophrastus)', 'olympia, greece')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_film[pairs[900][0]], index_link[pairs[900][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "KNq1cWk8ZvDC"
   },
   "outputs": [],
   "source": [
    "pairs_set = set(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "X8wFicaeZxiK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((13337, 31117), 85),\n",
       " ((31899, 71), 77),\n",
       " ((25899, 8857), 61),\n",
       " ((1851, 2635), 57),\n",
       " ((25899, 30471), 53)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Counter(pairs)\n",
    "sorted(x.items(), key = lambda x: x[1], reverse = True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "1yOfc0GVZ0Ql"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"France's Songs of the Bards of the Tyne - 1850\", 'animal testing')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('The Early Stories: 1953–1975', 'booklist')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('Marthandavarma (novel)', 'category:novels set in the 1760s')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_film[13337], index_link[31111]\n",
    "index_film[31899], index_link[65]\n",
    "index_film[25899], index_link[30465]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "bd103V6nZ9Ct"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "random.seed(100)\n",
    "\n",
    "def generate_batch(pairs, n_positive = 50, negative_ratio = 1.0, classification = False):\n",
    "    \"\"\"Generate batches of samples for training\"\"\"\n",
    "    batch_size = n_positive * (1 + negative_ratio)\n",
    "    batch = np.zeros((batch_size, 3))\n",
    "    \n",
    "    # Adjust label based on task\n",
    "    if classification:\n",
    "        neg_label = 0\n",
    "    else:\n",
    "        neg_label = -1\n",
    "    \n",
    "    # This creates a generator\n",
    "    while True:\n",
    "        # randomly choose positive examples\n",
    "        for idx, (film_id, link_id) in enumerate(random.sample(pairs, n_positive)):\n",
    "            batch[idx, :] = (film_id, link_id, 1)\n",
    "\n",
    "        # Increment idx by 1\n",
    "        idx += 1\n",
    "        \n",
    "        # Add negative examples until reach batch size\n",
    "        while idx < batch_size:\n",
    "            \n",
    "            # random selection\n",
    "            random_book = random.randrange(len(books))\n",
    "            random_link = random.randrange(len(links))\n",
    "            \n",
    "            # Check to make sure this is not a positive example\n",
    "            if (random_book, random_link) not in pairs_set:\n",
    "                \n",
    "                # Add to batch and increment index\n",
    "                batch[idx, :] = (random_book, random_link, neg_label)\n",
    "                idx += 1\n",
    "                \n",
    "        # Make sure to shuffle order\n",
    "        np.random.shuffle(batch)\n",
    "        yield {'film': batch[:, 0], 'link': batch[:, 1]}, batch[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "J0RJhSCjZ_ci"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'book': array([29814., 25757.,  6700.,  7206., 28410., 21518.]),\n",
       "  'link': array([11452., 22920., 15562., 34924., 33217.,  5097.])},\n",
       " array([-1., -1.,  1., -1., -1.,  1.]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(generate_batch(pairs, n_positive = 2, negative_ratio = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "wG36RQIbaBhT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book: The Soul of the Robot          Link: clinical psychology                      Label: -1.0\n",
      "Book: The Counterfeit Man            Link: summer                                   Label: -1.0\n",
      "Book: The Gods of Mars               Link: science fantasy                          Label: 1.0\n",
      "Book: Palace Walk                    Link: category:novels by naguib mahfouz        Label: 1.0\n",
      "Book: Soul Music (novel)             Link: mount olympus                            Label: -1.0\n",
      "Book: Des Imagistes                  Link: the odyssey                              Label: -1.0\n"
     ]
    }
   ],
   "source": [
    "x, y = next(generate_batch(pairs, n_positive = 2, negative_ratio = 2))\n",
    "\n",
    "# Show a few example training pairs\n",
    "for label, b_idx, l_idx in zip(y, x['film'], x['link']):\n",
    "    print(f'Film: {index_book[b_idx]:30} Link: {index_link[l_idx]:40} Label: {label}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ORQ10u4AaDhH"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, Dot, Reshape, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "lOih8f3haN_b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " book (InputLayer)              [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " link (InputLayer)              [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " book_embedding (Embedding)     (None, 1, 50)        1851000     ['book[0][0]']                   \n",
      "                                                                                                  \n",
      " link_embedding (Embedding)     (None, 1, 50)        2088200     ['link[0][0]']                   \n",
      "                                                                                                  \n",
      " dot_product (Dot)              (None, 1, 1)         0           ['book_embedding[0][0]',         \n",
      "                                                                  'link_embedding[0][0]']         \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 1)            0           ['dot_product[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,939,200\n",
      "Trainable params: 3,939,200\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def film_embedding_model(embedding_size = 50, classification = False):\n",
    "    \"\"\"Model to embed books and wikilinks using the functional API.\n",
    "       Trained to discern if a link is present in a article\"\"\"\n",
    "    \n",
    "    # Both inputs are 1-dimensional\n",
    "    film = Input(name = 'book', shape = [1])\n",
    "    link = Input(name = 'link', shape = [1])\n",
    "    \n",
    "    # Embedding the book (shape will be (None, 1, 50))\n",
    "    film_embedding = Embedding(name = 'film_embedding',\n",
    "                               input_dim = len(film_index),\n",
    "                               output_dim = embedding_size)(film)\n",
    "    \n",
    "    # Embedding the link (shape will be (None, 1, 50))\n",
    "    link_embedding = Embedding(name = 'link_embedding',\n",
    "                               input_dim = len(link_index),\n",
    "                               output_dim = embedding_size)(link)\n",
    "    \n",
    "    # Merge the layers with a dot product along the second axis (shape will be (None, 1, 1))\n",
    "    merged = Dot(name = 'dot_product', normalize = True, axes = 2)([film_embedding, link_embedding])\n",
    "    \n",
    "    # Reshape to be a single number (shape will be (None, 1))\n",
    "    merged = Reshape(target_shape = [1])(merged)\n",
    "    \n",
    "    # If classifcation, add extra layer and loss function is binary cross entropy\n",
    "    if classification:\n",
    "        merged = Dense(1, activation = 'sigmoid')(merged)\n",
    "        model = Model(inputs = [film, link], outputs = merged)\n",
    "        model.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    # Otherwise loss function is mean squared error\n",
    "    else:\n",
    "        model = Model(inputs = [film, link], outputs = merged)\n",
    "        model.compile(optimizer = 'Adam', loss = 'mse')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Instantiate model and show parameters\n",
    "model = film_embedding_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "nG8njLHUdn-L"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nikita\\AppData\\Local\\Temp\\ipykernel_17776\\3885179802.py:6: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  h = model.fit_generator(gen, epochs = 15,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "785/785 - 21s - loss: 0.9488 - 21s/epoch - 26ms/step\n",
      "Epoch 2/15\n",
      "785/785 - 20s - loss: 0.6437 - 20s/epoch - 26ms/step\n",
      "Epoch 3/15\n",
      "785/785 - 20s - loss: 0.4823 - 20s/epoch - 26ms/step\n",
      "Epoch 4/15\n",
      "785/785 - 20s - loss: 0.4514 - 20s/epoch - 26ms/step\n",
      "Epoch 5/15\n",
      "785/785 - 20s - loss: 0.4366 - 20s/epoch - 26ms/step\n",
      "Epoch 6/15\n",
      "785/785 - 20s - loss: 0.4276 - 20s/epoch - 26ms/step\n",
      "Epoch 7/15\n",
      "785/785 - 20s - loss: 0.4217 - 20s/epoch - 26ms/step\n",
      "Epoch 8/15\n",
      "785/785 - 20s - loss: 0.4170 - 20s/epoch - 26ms/step\n",
      "Epoch 9/15\n",
      "785/785 - 20s - loss: 0.4147 - 20s/epoch - 26ms/step\n",
      "Epoch 10/15\n",
      "785/785 - 20s - loss: 0.4114 - 20s/epoch - 26ms/step\n",
      "Epoch 11/15\n",
      "785/785 - 20s - loss: 0.4088 - 20s/epoch - 26ms/step\n",
      "Epoch 12/15\n",
      "785/785 - 20s - loss: 0.4076 - 20s/epoch - 26ms/step\n",
      "Epoch 13/15\n",
      "785/785 - 20s - loss: 0.4068 - 20s/epoch - 25ms/step\n",
      "Epoch 14/15\n",
      "785/785 - 20s - loss: 0.4053 - 20s/epoch - 25ms/step\n",
      "Epoch 15/15\n",
      "785/785 - 20s - loss: 0.4038 - 20s/epoch - 26ms/step\n"
     ]
    }
   ],
   "source": [
    "n_positive = 1024\n",
    "\n",
    "gen = generate_batch(pairs, n_positive, negative_ratio = 2)\n",
    "\n",
    "# Train\n",
    "h = model.fit_generator(gen, epochs = 15, \n",
    "                        steps_per_epoch = len(pairs) // n_positive,\n",
    "                        verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Z-CJK1cPeo-g"
   },
   "outputs": [],
   "source": [
    "model.save('..Диплом/model/first_try.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "dWoDvETRh7md"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37020, 50)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract embeddings\n",
    "film_layer = model.get_layer('film_embedding')\n",
    "film_weights = film_layer.get_weights()[0]\n",
    "film_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "yhBvXAQOiASJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.13010807, -0.16235666,  0.20284961, -0.05704584, -0.00856006,\n",
       "       -0.17848401, -0.04958921,  0.18384847, -0.03707412, -0.22918062],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.9999999"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "film_weights = film_weights / np.linalg.norm(film_weights, axis = 1).reshape((-1, 1))\n",
    "film_weights[0][:10]\n",
    "np.sum(np.square(film_weights[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Uu6Rhuj8iHUa"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [39]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams['font.size'] = 15\n",
    "\n",
    "def find_similar(name, weights, index_name = 'film', n = 10, least = False, return_dist = False, plot = False):\n",
    "    \"\"\"Find n most similar items (or least) to name based on embeddings. Option to also plot the results\"\"\"\n",
    "    \n",
    "    # Select index and reverse index\n",
    "    if index_name == 'film':\n",
    "        index = film_index\n",
    "        rindex = index_book\n",
    "    elif index_name == 'page':\n",
    "        index = link_index\n",
    "        rindex = index_link\n",
    "    \n",
    "    # Check to make sure `name` is in index\n",
    "    try:\n",
    "        # Calculate dot product between book and all others\n",
    "        dists = np.dot(weights, weights[index[name]])\n",
    "    except KeyError:\n",
    "        print(f'{name} Not Found.')\n",
    "        return\n",
    "    \n",
    "    # Sort distance indexes from smallest to largest\n",
    "    sorted_dists = np.argsort(dists)\n",
    "    \n",
    "    # Plot results if specified\n",
    "    if plot:\n",
    "        \n",
    "        # Find furthest and closest items\n",
    "        furthest = sorted_dists[:(n // 2)]\n",
    "        closest = sorted_dists[-n-1: len(dists) - 1]\n",
    "        items = [rindex[c] for c in furthest]\n",
    "        items.extend(rindex[c] for c in closest)\n",
    "        \n",
    "        # Find furthest and closets distances\n",
    "        distances = [dists[c] for c in furthest]\n",
    "        distances.extend(dists[c] for c in closest)\n",
    "        \n",
    "        colors = ['r' for _ in range(n //2)]\n",
    "        colors.extend('b' for _ in range(n))\n",
    "        \n",
    "        data = pd.DataFrame({'distance': distances}, index = items)\n",
    "        \n",
    "        # Horizontal bar chart\n",
    "        data['distance'].plot.barh(color = colors, figsize = (10, 8),\n",
    "                                   edgecolor = 'k', linewidth = 2)\n",
    "        plt.xlabel('Similiarity');\n",
    "        plt.axvline(x = 0, color = 'k');\n",
    "        \n",
    "        # Formatting for italicized title\n",
    "        name_str = f'{index_name.capitalize()}s Most and Least Similiar to'\n",
    "        for word in name.split():\n",
    "            # Title uses latex for italize\n",
    "            name_str += ' $\\it{' + word + '}$'\n",
    "        plt.title(name_str, x = 0.2, size = 28, y = 1.05)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    # If specified, find the least similar\n",
    "    if least:\n",
    "        # Take the first n from sorted distances\n",
    "        closest = sorted_dists[:n]\n",
    "         \n",
    "        print(f'{index_name.capitalize()}s furthest from {name}.\\n')\n",
    "        \n",
    "    # Otherwise find the most similar\n",
    "    else:\n",
    "        # Take the last n sorted distances\n",
    "        closest = sorted_dists[-n:]\n",
    "        \n",
    "        # Need distances later on\n",
    "        if return_dist:\n",
    "            return dists, closest\n",
    "        \n",
    "        \n",
    "        print(f'{index_name.capitalize()}s closest to {name}.\\n')\n",
    "        \n",
    "    # Need distances later on\n",
    "    if return_dist:\n",
    "        return dists, closest\n",
    "    \n",
    "    \n",
    "    # Print formatting\n",
    "    max_width = max([len(rindex[c]) for c in closest])\n",
    "    \n",
    "    # Print the most similar and distances\n",
    "    for c in reversed(closest):\n",
    "        print(f'{index_name.capitalize()}: {rindex[c]:{max_width + 2}} Similarity: {dists[c]:.{2}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "wKK1ZB3NiKBe"
   },
   "outputs": [],
   "source": [
    "find_similar('Coach Carter', film_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "tjKELU1piWaA"
   },
   "outputs": [],
   "source": [
    "find_similar('Coach Carter', film_weights, least = True, n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "iuRG1dsAibfL"
   },
   "outputs": [],
   "source": [
    "find_similar('Coach Carter', film_weights, n = 5, plot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_weights(name, model):\n",
    "    \"\"\"Extract weights from a neural network model\"\"\"\n",
    "    \n",
    "    # Extract weights\n",
    "    weight_layer = model.get_layer(name)\n",
    "    weights = weight_layer.get_weights()[0]\n",
    "    \n",
    "    # Normalize\n",
    "    weights = weights / np.linalg.norm(weights, axis = 1).reshape((-1, 1))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_weights = extract_weights('link_embedding', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar('fantasy', link_weights, index_name = 'page')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar('science fiction', link_weights, index_name = 'page', n = 5, plot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class = film_embedding_model(50, classification = True)\n",
    "gen = generate_batch(pairs, n_positive, negative_ratio=2, classification = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model to learn embeddings\n",
    "h = model_class.fit_generator(gen, epochs = 15, steps_per_epoch= len(pairs) // n_positive,\n",
    "                            verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class.save('..Диплом/models/first_attempt_class.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "film_weights_class = extract_weights('film_embedding', model_class)\n",
    "film_weights_class.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar('Thor: Ragnarok', book_weights_class, n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar('Coach Carter', book_weights_class, n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar('Parasite', book_weights_class, n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_weights_class = extract_weights('link_embedding', model_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar('Avengers', link_weights_class, index_name = 'page', n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar('category:western films', link_weights_class, index_name = 'page', n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar('Steven Spilberg', link_weights_class, index_name = 'page', n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar('Quentin Tarantino', link_weights_class, index_name = 'page', n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dim(weights, components = 3, method = 'tsne'):\n",
    "    \"\"\"Reduce dimensions of embeddings\"\"\"\n",
    "    if method == 'tsne':\n",
    "        return TSNE(components, metric = 'cosine').fit_transform(weights)\n",
    "    elif method == 'umap':\n",
    "        # Might want to try different parameters for UMAP\n",
    "        return UMAP(n_components=components, metric = 'cosine', \n",
    "                    init = 'random', n_neighbors = 5).fit_transform(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "film_r = reduce_dim(film_weights_class, components = 2, method = 'tsne')\n",
    "film_r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "InteractiveShell.ast_node_interactivity = 'last'\n",
    "\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.plot(film_r[:, 0], film_r[:, 1], 'r.')\n",
    "plt.xlabel('TSNE 1'); plt.ylabel('TSNE 2'); plt.title('Film Embeddings Visualized with TSNE');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "film_ru = reduce_dim(film_weights_class, components = 2, method = 'umap')\n",
    "\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.plot(film_ru[:, 0], film_ru[:, 1], 'g.');\n",
    "plt.xlabel('UMAP 1'); plt.ylabel('UMAP 2'); plt.title('Film Embeddings Visualized with UMAP');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = list(chain(*[set(film[1]) for film in films]))\n",
    "info_counts = count_items(info)\n",
    "list(info_counts.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = [film[1].get('genre', 'None').lower() for film in films]\n",
    "\n",
    "# Remove genres not found\n",
    "genre_counts = count_items(genres)\n",
    "del genre_counts['none']\n",
    "list(genre_counts.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include 10 most popular genres\n",
    "genre_to_include = list(genre_counts.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_include = []\n",
    "genres = []\n",
    "\n",
    "for i,film in enumerate(films):\n",
    "    if 'genre' in film[1].keys():\n",
    "        if film[1]['genre'].lower() in genre_to_include:\n",
    "            idx_include.append(i)\n",
    "            genres.append(film[1]['genre'].capitalize())\n",
    "            \n",
    "len(idx_include)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ints, gen = pd.factorize(genres)\n",
    "gen[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 8))\n",
    "\n",
    "# Plot embedding\n",
    "plt.scatter(film_r[idx_include, 0], film_r[idx_include, 1], \n",
    "            c = ints, cmap = plt.cm.tab10)\n",
    "\n",
    "# Add colorbar and appropriate labels\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_ticks([])\n",
    "for j, lab in enumerate(gen):\n",
    "    cbar.ax.text(1, (2 * j + 1) / ((10) * 2), lab, ha='left', va='center')\n",
    "cbar.ax.set_title('Genre', loc = 'left')\n",
    "\n",
    "\n",
    "plt.xlabel('TSNE 1'); plt.ylabel('TSNE 2'); plt.title('TSNE Visualization of Film Embeddings');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 8))\n",
    "\n",
    "# Plot embedding\n",
    "plt.scatter(film_ru[idx_include, 0], film_ru[idx_include, 1], \n",
    "            c = ints, cmap = plt.cm.tab10)\n",
    "\n",
    "# Add colorbar and appropriate labels\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_ticks([])\n",
    "for j, lab in enumerate(gen):\n",
    "    cbar.ax.text(1, (2 * j + 1) / ((10) * 2), lab, ha='left', va='center')\n",
    "cbar.ax.set_title('Genre', loc = 'left')\n",
    "\n",
    "\n",
    "plt.xlabel('UMAP 1'); plt.ylabel('UMAP 2'); plt.title('UMAP Visualization of Film Embeddings');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14, 12))\n",
    "\n",
    "# Plot all books\n",
    "plt.scatter(film_r[:, 0], film_r[:, 1], marker = '.', color = 'lightblue', alpha = 0.2)\n",
    "\n",
    "# Plot genres\n",
    "plt.scatter(film_r[idx_include, 0], film_r[idx_include, 1], \n",
    "            c = ints, cmap = plt.cm.tab10, alpha = 0.6)\n",
    "\n",
    "# Add colorbar and appropriate labels\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_ticks([])\n",
    "for j, lab in enumerate(gen):\n",
    "    cbar.ax.text(1, (2 * j + 1) / ((10) * 2), lab, ha='left', va='center')\n",
    "    \n",
    "cbar.ax.set_title('Genre', loc = 'left')\n",
    "plt.xlabel('TSNE 1'); plt.ylabel('TSNE 2'); plt.title('TSNE Visualization of Film Embeddings');\n",
    "\n",
    "for film in list(wikilink_film_counts.keys())[:10]:\n",
    "    \n",
    "    x, y = film_r[film_index[film], 0], film_r[film_index[film], 1];\n",
    "    # Italize film title using latex\n",
    "    s =  ''.join([' $\\it{' + word + '}$' for word in film.split()])\n",
    "    _ = plt.scatter(x, y, s = 250, color = 'r',\n",
    "                    marker = '*', edgecolor = 'k')\n",
    "    _ = plt.text(x - 10, y + 2, s, fontsize = 14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "film_r[film_index['']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_by_attribute(attribute):\n",
    "    \"\"\"Color film embedding by `attribute`\"\"\"\n",
    "    # Find all the attribute values\n",
    "    attrs = [film[1].get(attribute, 0) for film in films]\n",
    "\n",
    "    # Remove attributes not found\n",
    "    attr_counts = count_items(attrs)\n",
    "    del attr_counts[0]\n",
    "    \n",
    "    # Include 10 most popular attributes\n",
    "    attr_to_include, counts = list(attr_counts.keys())[:10], list(attr_counts.values())[:10]\n",
    "    \n",
    "    idx_include = []\n",
    "    attributes = []\n",
    "\n",
    "    # Iterate through books searching for the attribute\n",
    "    for i, film in enumerate(films):\n",
    "        # Limit to films with the attribute\n",
    "        if attribute in film[1].keys():\n",
    "            # Limit to attribute in the 10 most popular\n",
    "            if film[1][attribute] in attr_to_include:\n",
    "                idx_include.append(i)\n",
    "                attributes.append(film[1][attribute])\n",
    "                \n",
    "    # Map to integers\n",
    "    ints, attrs = pd.factorize(attributes)\n",
    "    plt.figure(figsize = (12, 10))\n",
    "\n",
    "    plt.scatter(film_r[:, 0], film_r[:, 1], marker = '.', color = 'lightblue', alpha = 0.2)\n",
    "    \n",
    "    # Plot embedding with only specific attribute highlighted\n",
    "    plt.scatter(film_r[idx_include, 0], film_r[idx_include, 1], alpha = 0.6,\n",
    "                c = ints, cmap = plt.cm.tab10, marker = 'o', s = 50)\n",
    "\n",
    "    # Add colorbar and appropriate labels\n",
    "    cbar = plt.colorbar()\n",
    "    cbar.set_ticks([])\n",
    "    tick_labels = [f'{attr}: {count}' for attr, count in zip(attr_to_include, counts)]\n",
    "    # Labeling\n",
    "    for j, lab in enumerate(tick_labels):\n",
    "        cbar.ax.text(1, (2 * j + 1) / ((10) * 2), lab, ha='left', va='center')\n",
    "    cbar.ax.set_title(f'{attribute.capitalize()}: Count', loc = 'left')\n",
    "\n",
    "\n",
    "    plt.xlabel('TSNE 1'); plt.ylabel('TSNE 2'); plt.title(f'Film Embeddings with {attribute.capitalize()}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_by_attribute('genre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_by_attribute('author')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_by_attribute('country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_by_attribute('language')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNMbVzKHCW4gGiJsu5onl0o",
   "name": "RecomSys Neural Networks.ipynb",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
